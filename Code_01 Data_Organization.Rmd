---
title: "Flow Data Organization"
author: "Kyle Chezik"
date: "June 24, 2015"
output: pdf_document
---

###Load Original Raw Data.
```{r, message=FALSE, cache=TRUE, include=FALSE}
#Load in libraries.
library(plyr);library(tidyverse);library(MARSS);library(zoo);library(lubridate)
```

###Subset and Constrain Data
```{r, echo=FALSE, message=F, dependson=1}
load("01_Data-Flow-Orig.RData")
# Remove Dam Influenced Sites.
Flow.2 = filter(Flow.2, !(Station.ID %in% c("08JC001","08JC002","08ME002")))

#Limit the HYDAT flow data to years between 1970 and 2007.
Data = Flow.2 %>% filter(year(Date)>=1970, year(Date)<=2007)

#Split up the date for processesing.
Data = Data %>% mutate(Year = year(Date), nMonth = month(Date), Month = month(Date, label = T), Day = day(Date), DOY = yday(Date))

#Determine the number of days missing in each month of each year by flow gauge station (DIMM = Days In Month Missing). Remove months with more then 5 days missing.
DIMM = Data %>% group_by(Station.ID, Year, Month) %>%
	summarize(Days.Missing = unique(days_in_month(Date)-n())) %>% 
	filter(Days.Missing<=5) %>% ungroup()

#Determine the number of months in each year for each flow gauge site. Remove years with fewer than 12 months.
DIM.Limited.Months = DIMM %>% group_by(Station.ID, Year) %>% 
	summarize(Months = n()) %>% filter(Months == 12)

#Isolate data with Years that have 12 months in which no month is missing more than 5 days.
Data = plyr::match_df(Data,DIM.Limited.Months,on = c("Station.ID","Year"))

#Determine the number of years included in each site. Remove stations with fewer than 35 years of data.
Stations.Final = DIM.Limited.Months %>% summarise(Years = n()) %>% 
	filter(Years>=35)

#Isolate data such that all included sites have at least 35 of the possible 38 years between 1970 and 2007, and each year has 12 full months where each month is missing no more than five days.
Data = plyr::match_df(Data,Stations.Final,on = "Station.ID")
save(Data,file = "02a_Data_Clean.RData")
```

###Interpolate Mising Data
```{r,echo=F,message=F, dependson=1}
#Fill in missing data for the 'day of year' analysis.
load("02a_Data_Clean.RData") #Load processesed data.

#Split up the date for processesing.
Data = Data %>% mutate(Year = year(Date), nMonth = month(Date), Month = month(Date, label = T), Day = day(Date), DOY = yday(Date))

#Determine the number of missing days in each Station, Year, Month combination. Retain those that have more than 0.
Missing = Data %>% group_by(Station.ID, Year, nMonth) %>% 
	summarize(Days.Missing = unique(days_in_month(Date)-n())) %>% 
	filter(Days.Missing > 0)

library(doParallel)
registerDoParallel(cores = 7)
#Create a list for each station and year combination where a month is missing data. These lists include data for the year prior and after the year in which data is missing. This creates a vector/dataset in the wide format which is necessary for the auto-regressive state-space model.
Sites = plyr::dlply(Missing,c("Station.ID","Year"),function(x){
	#browser()
	Period = seq(unique(x$Year)-1,unique(x$Year+1))
	Site = subset(Data, Station.ID==x$Station.ID & Year%in%Period)
	Potential.Dates = seq(as.Date(paste(Period[1],"-01-01",sep="")), as.Date(paste(Period[3],"-12-31",sep="")), by = "day")
	Date.Rule = data.frame(Station.ID = rep(unique(x$Station.ID),length(Potential.Dates)),Date = Potential.Dates)
	Site.Missing = full_join(Site, Date.Rule, by = c("Date","Station.ID"))
	Site = spread(Site.Missing[,c(1:3)],Date,Flow.Data)[1,]
	as.numeric(as.vector(Site[,c(2:ncol(Site))]))
})

pdf("interpolation-checks-approx.pdf", width = 9, height = 6)
par(mfrow = c(3, 3), mar = c(3, 3, 1, 1), cex = 0.5)
plyr::l_ply(Sites, function(i) {
	ind <- seq_along(i)
	d <- data.frame(flow = i, ind = ind)
	d$approx <- approx(d$ind[!is.na(d$flow)], y = log(d$flow[!is.na(d$flow)]),
		xout = d$ind)$y %>% exp()
  plot(d$ind, d$flow, type = "o", pch = 20, cex = 0.6)
  # lines(d$ind, d$approx, col = "blue")
  pred_nas <- d[which(is.na(d$flow)), c("ind", "approx")]
  points(pred_nas$ind, pred_nas$approx, col = "red", cex = 1, pch = 20)
})
dev.off()

# Interpolate the missing values via a linear interpolation (in log space):
Preds = plyr::ldply(names(Sites),function(x){
	Year = as.numeric(strsplit(x,split = "\\.")[[1]][2])
	Site = strsplit(x,split = "\\.")[[1]][1]
	Potential.Dates = seq(as.Date(paste(Year-1,"-01-01",sep="")), as.Date(paste(Year+1,"-12-31",sep="")), by = "day")
	x.Dates = Data[which(Data$Station.ID==Site & Data$Year==Year),"Date"]
	rows = which(Potential.Dates%in%x.Dates==F & as.numeric(format(Potential.Dates,"%Y"))==Year)
	
	ind <- seq_along(Sites[[x]])
	d <- data.frame(flow = Sites[[x]], ind = ind)
	d$approx <- approx(d$ind[!is.na(d$flow)], y = log(d$flow[!is.na(d$flow)]),
		xout = d$ind)$y %>% exp()
	d <- d[rows, ]
	
	data.frame(Station.ID = rep(Site,length(rows)),
		Date = Potential.Dates[rows],
		Flow.Data = d$approx)
})

#Split up the date for processesing.
Preds = Preds %>% mutate(Year = year(Date), nMonth = month(Date), Month = month(Date, label = T), Day = day(Date), DOY = yday(Date))

Data.Preds = bind_rows(Data, Preds)
save(Data.Preds, file = "02b_Missing_Data_Predictions.RData")
```

###Create Annual Dataset
```{r,echo=F,message=F,dependson=1}
#Load Data
load("01_Data-Wtshd-Orig.RData"); names(WS.2) = c("Station.ID","Lat","Long","Area")
load("02b_Missing_Data_Predictions.RData")

#Calculate Rolling Average to smooth out daily extremes.
flow.stats = Data.Preds %>% group_by(Station.ID) %>% 
	arrange(Date) %>%
	do({
		z = zoo(.$Flow.Data, .$Date)
		x = rollapply(data = z, width = 5, by = 1, FUN = mean)
		data.frame(Date = index(x), mean5day = coredata(x))
	})

#Summarise annual data by the day of year to half annual flow.
# Get the row numbers of when cumulative flow volume hits half the total yearly volume:
accum_flow <- Data.Preds %>% left_join(., flow.stats, by  = c("Station.ID", "Date")) %>% 
	mutate(row_number = 1:length(Year)) %>%
	group_by(Station.ID, Year) %>%
	mutate(half_total_flow = sum(Flow.Data) / 2) %>%
	summarise(row_number = max(row_number[cumsum(Flow.Data) < half_total_flow])) %>%
	as.data.frame()
# Select those rows:
accum_flow <- Data.Preds[accum_flow$row_number, ] %>%
	inner_join(accum_flow) %>%
	select(-row_number)

# Summarize flow data.
flow.stats = left_join(Data.Preds, flow.stats, by = c("Station.ID","Date")) %>% 
	group_by(Station.ID, Year) %>% 
	summarise(Median.F = median(Flow.Data),
									 Min.F = min(mean5day, na.rm = T),
									 Max.F = max(mean5day, na.rm = T))

Y.Data = left_join(accum_flow, flow.stats, by = c("Station.ID","Year"))
Y.Data = left_join(Y.Data, WS.2, by = c("Station.ID"))
names(Y.Data)[8] = "DOY2"
save(Y.Data,file = "03_Data_Annual.RData")
```

###Create Monthly Dataset
```{r,echo=F,message=F,dependson=1}
#Summarise data monthly mean, minnimum and maximums.
load("01_Data-Wtshd-Orig.RData"); names(WS.2) = c("Station.ID","Lat","Long","Area")
load("02b_Missing_Data_Predictions.RData")
df = Data.Preds %>% group_by(Station.ID) %>% arrange(Date) %>%
	do({
		z = zoo(.$Flow.Data, .$Date)
		x = rollapply(data = z, width = 5, by = 1, FUN = mean)
		data.frame(Date = index(x), mean5day = coredata(x))
	})

M.Data = left_join(Data.Preds, df, by = c("Station.ID","Date")) %>% 
	mutate(flow.data_mean = if_else(is.na(.$mean5day), .$Flow.Data, .$mean5day)) %>%
	select(-mean5day) %>% 
	group_by(Station.ID, Year, nMonth, Month) %>% 
	summarise(median.M.flow = median(flow.data_mean),
 						max.M.flow = max(flow.data_mean, na.rm = T),
 						min.M.flow = min(flow.data_mean, na.rm = T))
#Join location and area data to flow data.
M.Data = left_join(M.Data, WS.2, by = "Station.ID")
save(M.Data,file = "03_Data_Monthly.RData")
```